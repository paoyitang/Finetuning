{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**拉取训练数据集**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_dataset = load_dataset(\"EleutherAI/pile\", split=\"train\", streaming=True, trust_remote_code=True) #streaming 顺序流式传输"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**加载问答对数据集**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How can I evaluate the performance and quality...</td>\n",
       "      <td>There are several metrics that can be used to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can I find information about the code's approa...</td>\n",
       "      <td>Yes, the code includes methods for submitting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does Lamini AI handle requests for generat...</td>\n",
       "      <td>Lamini AI offers features for generating text ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Does the `submit_job()` function expose any ad...</td>\n",
       "      <td>It is unclear which `submit_job()` function is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Does the `add_data()` function support differe...</td>\n",
       "      <td>No, the `add_data()` function does not support...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>Does Lamini have the ability to understand and...</td>\n",
       "      <td>Yes, Lamini has the ability to understand and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>Can I fine-tune the pre-trained models provide...</td>\n",
       "      <td>Yes, you can fine-tune the pre-trained models ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>Can Lamini generate text that is suitable for ...</td>\n",
       "      <td>Yes, Lamini can generate text that is suitable...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>Does the documentation have a secret code that...</td>\n",
       "      <td>I wish! This documentation only talks about La...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>Can I teach Lamini about things that I like or...</td>\n",
       "      <td>Absolutely! One of the fascinating aspects of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  \\\n",
       "0     How can I evaluate the performance and quality...   \n",
       "1     Can I find information about the code's approa...   \n",
       "2     How does Lamini AI handle requests for generat...   \n",
       "3     Does the `submit_job()` function expose any ad...   \n",
       "4     Does the `add_data()` function support differe...   \n",
       "...                                                 ...   \n",
       "1395  Does Lamini have the ability to understand and...   \n",
       "1396  Can I fine-tune the pre-trained models provide...   \n",
       "1397  Can Lamini generate text that is suitable for ...   \n",
       "1398  Does the documentation have a secret code that...   \n",
       "1399  Can I teach Lamini about things that I like or...   \n",
       "\n",
       "                                                 answer  \n",
       "0     There are several metrics that can be used to ...  \n",
       "1     Yes, the code includes methods for submitting ...  \n",
       "2     Lamini AI offers features for generating text ...  \n",
       "3     It is unclear which `submit_job()` function is...  \n",
       "4     No, the `add_data()` function does not support...  \n",
       "...                                                 ...  \n",
       "1395  Yes, Lamini has the ability to understand and ...  \n",
       "1396  Yes, you can fine-tune the pre-trained models ...  \n",
       "1397  Yes, Lamini can generate text that is suitable...  \n",
       "1398  I wish! This documentation only talks about La...  \n",
       "1399  Absolutely! One of the fascinating aspects of ...  \n",
       "\n",
       "[1400 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(\"hf://datasets/kotzeje/lamini_docs.jsonl/data/train-00000-of-00001-6359aa989b671345.parquet\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**将问答对转变成字典的格式**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"How can I evaluate the performance and quality of the generated text from Lamini models?There are several metrics that can be used to evaluate the performance and quality of generated text from Lamini models, including perplexity, BLEU score, and human evaluation. Perplexity measures how well the model predicts the next word in a sequence, while BLEU score measures the similarity between the generated text and a reference text. Human evaluation involves having human judges rate the quality of the generated text based on factors such as coherence, fluency, and relevance. It is recommended to use a combination of these metrics for a comprehensive evaluation of the model's performance.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = df.to_dict()\n",
    "text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_qa = \"\"\"### Question:\n",
    "{question}\n",
    "\n",
    "### Answer:\n",
    "{answer}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"### Question:\\nHow can I evaluate the performance and quality of the generated text from Lamini models?\\n\\n### Answer:\\nThere are several metrics that can be used to evaluate the performance and quality of generated text from Lamini models, including perplexity, BLEU score, and human evaluation. Perplexity measures how well the model predicts the next word in a sequence, while BLEU score measures the similarity between the generated text and a reference text. Human evaluation involves having human judges rate the quality of the generated text based on factors such as coherence, fluency, and relevance. It is recommended to use a combination of these metrics for a comprehensive evaluation of the model's performance.\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = examples[\"question\"][0]\n",
    "answer = examples[\"answer\"][0]\n",
    "\n",
    "text_with_prompt_template = prompt_template_qa.format(question=question, answer=answer)\n",
    "text_with_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_q = \"\"\"### Question:\n",
    "{question}\n",
    "\n",
    "### Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = len(examples[\"question\"])\n",
    "finetuning_dataset_text_only =[]\n",
    "finetuning_dataset_question_answer = []\n",
    "for i in range(num_examples):\n",
    "    question = examples[\"question\"][i]\n",
    "    answer = examples[\"answer\"][i]\n",
    "\n",
    "    text_with_prompt_template_qa = prompt_template_qa.format(question=question, answer=answer)\n",
    "    finetuning_dataset_text_only.append({\"text\": text_with_prompt_template_qa})\n",
    "\n",
    "    text_with_prompt_template_q = prompt_template_q.format(question=question)\n",
    "    finetuning_dataset_question_answer.append({\"question\": text_with_prompt_template_q, \"answer\": text_with_prompt_template_q})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**纯文本格式**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '### Question:\\n'\n",
      "         'How can I evaluate the performance and quality of the generated text '\n",
      "         'from Lamini models?\\n'\n",
      "         '\\n'\n",
      "         '### Answer:\\n'\n",
      "         'There are several metrics that can be used to evaluate the '\n",
      "         'performance and quality of generated text from Lamini models, '\n",
      "         'including perplexity, BLEU score, and human evaluation. Perplexity '\n",
      "         'measures how well the model predicts the next word in a sequence, '\n",
      "         'while BLEU score measures the similarity between the generated text '\n",
      "         'and a reference text. Human evaluation involves having human judges '\n",
      "         'rate the quality of the generated text based on factors such as '\n",
      "         'coherence, fluency, and relevance. It is recommended to use a '\n",
      "         'combination of these metrics for a comprehensive evaluation of the '\n",
      "         \"model's performance.\"}\n"
     ]
    }
   ],
   "source": [
    "pprint(finetuning_dataset_text_only[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题-答案格式**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': '### Question:\\n'\n",
      "           'How can I evaluate the performance and quality of the generated '\n",
      "           'text from Lamini models?\\n'\n",
      "           '\\n'\n",
      "           '### Answer:',\n",
      " 'question': '### Question:\\n'\n",
      "             'How can I evaluate the performance and quality of the generated '\n",
      "             'text from Lamini models?\\n'\n",
      "             '\\n'\n",
      "             '### Answer:'}\n"
     ]
    }
   ],
   "source": [
    "pprint(finetuning_dataset_question_answer[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**将处理后的数据保存为标准化格式**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open(f'lamini_docs_processed.jsonl', 'w') as writer:\n",
    "    writer.write_all(finetuning_dataset_question_answer) #?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**代码运行原理**\n",
    "\n",
    "JSON Lines 是一种轻量级数据交换格式，每行是一个独立的 JSON 对象。\n",
    "\n",
    "1.jsonlines.open()：\n",
    "\n",
    "使用 jsonlines 库打开文件。\n",
    "'w' 表示写入模式（若文件已存在，则覆盖；若不存在，则创建）。\n",
    "with 语句确保文件操作完成后自动关闭，避免资源泄漏。\n",
    "\n",
    "2.writer.write_all():\n",
    "\n",
    "将 finetuning_dataset_question_answer（一个列表，包含多个字典）中的所有数据逐行写入文件。\n",
    "\n",
    "3.读取文件\n",
    "\n",
    "with jsonlines.open('lamini_docs_processed.jsonl', 'r') as reader:\n",
    "    data = [item for item in reader]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**加载来自Hugging Face的数据集**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c75537e9554834ab0b02ca1199912d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-5cdebbc48da41394.parquet:   0%|          | 0.00/615k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87839445464a4afa875db59af56f8d7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-4c77a066a883f339.parquet:   0%|          | 0.00/83.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68717818c11e4e0695e56beaf792d73a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1260 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01a6fd222302431eb4f6c168714c69cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1260\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 140\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "finetuning_dataset_name = \"lamini/lamini_docs\"\n",
    "finetuning_dataset = load_dataset(finetuning_dataset_name)\n",
    "print(finetuning_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
